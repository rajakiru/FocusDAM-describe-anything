{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FocusDAM: Baseline Replication on DLC-Bench\n",
    "\n",
    "**Course**: 10-623 Generative AI  \n",
    "**Team**: Saksham Bhutani, Smriti Jha, Kiruthika Raja  \n",
    "**Date**: November 17, 2025\n",
    "\n",
    "## Notebook Overview\n",
    "\n",
    "This notebook replicates the DAM-3B baseline results on DLC-Bench:\n",
    "1. Setup environment and mount Google Drive\n",
    "2. Install dependencies (DAM package, vLLM)\n",
    "3. Download DLC-Bench dataset\n",
    "4. Run DAM-3B inference to generate captions\n",
    "5. Evaluate with LLM judge (Llama-3.1-8B)\n",
    "6. Verify baseline scores (expected ~0.67)\n",
    "\n",
    "**Expected Runtime**: 2-3 hours on A100 (first run with downloads)  \n",
    "**VRAM Usage**: ~20-25GB (DAM-3B inference) + ~16GB (vLLM evaluation)  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup\n",
    "\n",
    "**IMPORTANT**: Ensure you are using A100 GPU runtime:  \n",
    "`Runtime > Change runtime type > A100 GPU`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi\n",
    "\n",
    "# Verify A100\n",
    "import torch\n",
    "print(f\"\\nGPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "assert \"A100\" in torch.cuda.get_device_name(0), \"Please switch to A100 GPU runtime.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Mount Google Drive\n",
    "\n",
    "Mount Google Drive to persist datasets and outputs across sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create project directory structure\n",
    "PROJECT_ROOT = '/content/drive/MyDrive/FocusDAM-project'\n",
    "os.makedirs(f\"{PROJECT_ROOT}/datasets\", exist_ok=True)\n",
    "os.makedirs(f\"{PROJECT_ROOT}/models\", exist_ok=True)\n",
    "os.makedirs(f\"{PROJECT_ROOT}/outputs\", exist_ok=True)\n",
    "os.makedirs(f\"{PROJECT_ROOT}/results\", exist_ok=True)\n",
    "\n",
    "print(f\"Project directory created at: {PROJECT_ROOT}\")\n",
    "print(f\"Directory structure:\")\n",
    "!ls -la {PROJECT_ROOT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Clone Repository and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repo to local Colab storage\n",
    "%cd /content\n",
    "!git clone https://github.com/NVlabs/describe-anything.git\n",
    "%cd describe-anything\n",
    "\n",
    "print(\"\\nInstalling DAM package...\")\n",
    "!pip install -e . -q\n",
    "\n",
    "print(\"\\nInstallation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Download DLC-Bench Dataset\n",
    "\n",
    "Downloads approximately 2GB of data. Cached in Google Drive for future sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DLC_BENCH_PATH = f\"{PROJECT_ROOT}/datasets/DLC-bench\"\n",
    "\n",
    "# Check if already downloaded\n",
    "if os.path.exists(f\"{DLC_BENCH_PATH}/annotations.json\"):\n",
    "    print(f\"DLC-Bench already downloaded at: {DLC_BENCH_PATH}\")\n",
    "    # Create symlink in evaluation folder\n",
    "    !ln -sf {DLC_BENCH_PATH} /content/describe-anything/evaluation/DLC-bench\n",
    "else:\n",
    "    print(\"Downloading DLC-Bench dataset (~2GB)...\")\n",
    "    %cd {PROJECT_ROOT}/datasets\n",
    "    !git lfs install\n",
    "    !git clone https://huggingface.co/datasets/nvidia/DLC-Bench\n",
    "    !mv DLC-Bench DLC-bench\n",
    "    \n",
    "    # Create symlink\n",
    "    %cd /content/describe-anything/evaluation\n",
    "    !ln -sf {DLC_BENCH_PATH} DLC-bench\n",
    "    \n",
    "    print(f\"\\nDLC-Bench downloaded and cached at: {DLC_BENCH_PATH}\")\n",
    "\n",
    "# Verify dataset\n",
    "print(\"\\nDataset structure:\")\n",
    "!ls -lh /content/describe-anything/evaluation/DLC-bench/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Load DAM-3B Model\n",
    "\n",
    "Downloads the pretrained model from HuggingFace (~6GB). Cached for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = f\"{PROJECT_ROOT}/models\"\n",
    "\n",
    "from dam import DescribeAnythingModel\n",
    "import torch\n",
    "\n",
    "print(\"Loading DAM-3B model from HuggingFace...\")\n",
    "print(\"(This may take 5-10 minutes on first run)\\n\")\n",
    "\n",
    "model = DescribeAnythingModel(\n",
    "    model_path=\"nvidia/DAM-3B\",\n",
    "    conv_mode=\"v1\",\n",
    "    prompt_mode=\"full+focal_crop\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "print(\"\\nDAM-3B model loaded successfully.\")\n",
    "print(f\"Model size: {sum(p.numel() for p in model.model.parameters()) / 1e9:.2f}B parameters\")\n",
    "print(f\"VRAM usage: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Test Inference on Sample Image\n",
    "\n",
    "Verify the model works correctly before running full evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pycocotools.coco import COCO\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%cd /content/describe-anything/evaluation\n",
    "\n",
    "# Load dataset\n",
    "coco = COCO('DLC-bench/annotations.json')\n",
    "\n",
    "# Get first image\n",
    "img_ids = sorted(coco.getImgIds())\n",
    "img_info = coco.loadImgs(img_ids[0])[0]\n",
    "img_path = f\"DLC-bench/{img_info['file_name']}\"\n",
    "image_pil = Image.open(img_path).convert('RGB')\n",
    "\n",
    "# Get first annotation (mask)\n",
    "ann_ids = coco.getAnnIds(imgIds=img_info['id'])\n",
    "ann = coco.loadAnns(ann_ids[0])[0]\n",
    "mask_np = coco.annToMask(ann)\n",
    "mask_pil = Image.fromarray(mask_np * 255)\n",
    "\n",
    "# Run inference\n",
    "query = \"<image>\\nDescribe the masked region in detail.\"\n",
    "print(\"Generating description...\\n\")\n",
    "\n",
    "description = model.get_description(\n",
    "    image_pil, \n",
    "    mask_pil, \n",
    "    query,\n",
    "    temperature=0.2,\n",
    "    top_p=0.9,\n",
    "    num_beams=1,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "axes[0].imshow(image_pil)\n",
    "axes[0].set_title('Original Image')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(mask_np, cmap='gray')\n",
    "axes[1].set_title('Region Mask')\n",
    "axes[1].axis('off')\n",
    "\n",
    "# Overlay mask on image\n",
    "overlay = np.array(image_pil).copy()\n",
    "overlay[mask_np > 0] = overlay[mask_np > 0] * 0.5 + np.array([255, 0, 0]) * 0.5\n",
    "axes[2].imshow(overlay.astype(np.uint8))\n",
    "axes[2].set_title('Masked Region')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{PROJECT_ROOT}/results/sample_inference.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GENERATED DESCRIPTION:\")\n",
    "print(\"=\"*80)\n",
    "print(description)\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Generate Captions on Full DLC-Bench\n",
    "\n",
    "**Note**: This will take approximately 1-2 hours to generate captions for all images.  \n",
    "Outputs are saved to Google Drive to prevent data loss if session disconnects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/describe-anything/evaluation\n",
    "\n",
    "# Check if outputs already exist\n",
    "output_path = f\"{PROJECT_ROOT}/outputs/baseline_dam3b_outputs.json\"\n",
    "\n",
    "if os.path.exists(output_path):\n",
    "    print(f\"Baseline outputs already exist at: {output_path}\")\n",
    "    print(\"Skipping inference. Delete file to re-run.\")\n",
    "else:\n",
    "    print(\"Running full DAM-3B inference on DLC-Bench...\")\n",
    "    print(\"This will take approximately 1-2 hours.\\n\")\n",
    "    \n",
    "    !python get_model_outputs.py \\\n",
    "        --model_type dam \\\n",
    "        --model_path nvidia/DAM-3B \\\n",
    "        --conv-mode v1 \\\n",
    "        --crop-mode full+focal_crop \\\n",
    "        --temperature 0.2 \\\n",
    "        --query \"<image>\\nDescribe the masked region in detail.\"\n",
    "    \n",
    "    # Copy outputs to Google Drive\n",
    "    !cp model_outputs_cache/dam_*.json {output_path}\n",
    "    print(f\"\\nOutputs saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Install vLLM for Evaluation\n",
    "\n",
    "vLLM is required to run Llama-3.1-8B as the LLM judge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install vLLM\n",
    "print(\"Installing vLLM...\")\n",
    "!pip install vllm==0.5.3.post1 -q\n",
    "\n",
    "# Install OpenAI client\n",
    "!pip install openai inflect -q\n",
    "\n",
    "print(\"vLLM installed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Start vLLM Server\n",
    "\n",
    "**IMPORTANT**: This starts Llama-3.1-8B in the background.  \n",
    "You may need to restart runtime after inference to free VRAM before running this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import requests\n",
    "\n",
    "# Clear GPU memory first\n",
    "import gc\n",
    "import torch\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"VRAM freed. Current usage: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\\n\")\n",
    "\n",
    "# Start vLLM server\n",
    "print(\"Starting vLLM server with Llama-3.1-8B...\")\n",
    "print(\"(This may take 3-5 minutes to load model)\\n\")\n",
    "\n",
    "vllm_process = subprocess.Popen([\n",
    "    \"vllm\", \"serve\", \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    \"--port\", \"9000\",\n",
    "    \"--max-model-len\", \"8192\",\n",
    "    \"--gpu-memory-utilization\", \"0.8\"\n",
    "], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "# Wait for server to start\n",
    "max_wait = 300  # 5 minutes\n",
    "start_time = time.time()\n",
    "while time.time() - start_time < max_wait:\n",
    "    try:\n",
    "        response = requests.get(\"http://localhost:9000/health\")\n",
    "        if response.status_code == 200:\n",
    "            print(\"vLLM server is ready.\")\n",
    "            break\n",
    "    except:\n",
    "        pass\n",
    "    time.sleep(5)\n",
    "    print(\"Waiting for vLLM server to start...\")\n",
    "else:\n",
    "    print(\"Warning: Server did not start within expected time. Check logs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Evaluate with LLM Judge\n",
    "\n",
    "Evaluates generated captions using Llama-3.1-8B as a judge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/describe-anything/evaluation\n",
    "\n",
    "# Copy outputs from Drive to local cache\n",
    "!mkdir -p model_outputs_cache\n",
    "!cp {PROJECT_ROOT}/outputs/baseline_dam3b_outputs.json model_outputs_cache/\n",
    "\n",
    "# Run evaluation\n",
    "print(\"Evaluating baseline outputs with LLM judge...\\n\")\n",
    "\n",
    "!python eval_model_outputs.py \\\n",
    "    --pred model_outputs_cache/baseline_dam3b_outputs.json \\\n",
    "    --base-url \"http://localhost:9000/v1\" \\\n",
    "    | tee {PROJECT_ROOT}/results/baseline_eval_log.txt\n",
    "\n",
    "print(\"\\nEvaluation complete.\")\n",
    "print(f\"Full log saved to: {PROJECT_ROOT}/results/baseline_eval_log.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Analyze Results\n",
    "\n",
    "Parse and display evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Read evaluation log\n",
    "with open(f\"{PROJECT_ROOT}/results/baseline_eval_log.txt\", 'r') as f:\n",
    "    log = f.read()\n",
    "\n",
    "# Extract scores\n",
    "match = re.search(r'Summary.*?:\\s*([0-9.]+),\\s*([0-9.]+),\\s*([0-9.]+)', log)\n",
    "if match:\n",
    "    pos_score = float(match.group(1))\n",
    "    neg_score = float(match.group(2))\n",
    "    avg_score = float(match.group(3))\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"BASELINE REPLICATION RESULTS (DAM-3B)\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nDLC-Bench LLM Judge Scores:\")\n",
    "    print(f\"   Positive (Correctness):     {pos_score:.3f}\")\n",
    "    print(f\"   Negative (Locality):        {neg_score:.3f}\")\n",
    "    print(f\"   Average:                    {avg_score:.3f}\")\n",
    "    \n",
    "    print(f\"\\nReference (from paper):\")\n",
    "    print(f\"   Positive: 0.510\")\n",
    "    print(f\"   Negative: 0.830\")\n",
    "    print(f\"   Average:  0.670\")\n",
    "    \n",
    "    # Check if replication is successful\n",
    "    if abs(avg_score - 0.670) < 0.05:\n",
    "        print(f\"\\nBASELINE SUCCESSFULLY REPLICATED.\")\n",
    "        print(f\"Your score ({avg_score:.3f}) matches reference (0.670).\")\n",
    "    else:\n",
    "        print(f\"\\nNote: Score differs from reference.\")\n",
    "        print(f\"Difference: {abs(avg_score - 0.670):.3f}\")\n",
    "        print(f\"This may be due to model updates or randomness.\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Save results to JSON\n",
    "    import json\n",
    "    results = {\n",
    "        \"model\": \"DAM-3B\",\n",
    "        \"dataset\": \"DLC-Bench\",\n",
    "        \"scores\": {\n",
    "            \"positive\": pos_score,\n",
    "            \"negative\": neg_score,\n",
    "            \"average\": avg_score\n",
    "        },\n",
    "        \"reference\": {\n",
    "            \"positive\": 0.510,\n",
    "            \"negative\": 0.830,\n",
    "            \"average\": 0.670\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(f\"{PROJECT_ROOT}/results/baseline_scores.json\", 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nResults saved to: {PROJECT_ROOT}/results/baseline_scores.json\")\n",
    "else:\n",
    "    print(\"Warning: Could not parse evaluation results. Check log file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "### Completed Tasks:\n",
    "1. Environment setup on Colab A100\n",
    "2. Downloaded DLC-Bench dataset (~2GB)\n",
    "3. Loaded DAM-3B model (~6GB)\n",
    "4. Generated captions for all DLC-Bench images\n",
    "5. Evaluated with LLM judge (Llama-3.1-8B)\n",
    "6. Verified baseline scores\n",
    "\n",
    "### Files Created:\n",
    "- `{PROJECT_ROOT}/datasets/DLC-bench/` - Dataset (cached)\n",
    "- `{PROJECT_ROOT}/models/` - Model checkpoints (cached)\n",
    "- `{PROJECT_ROOT}/outputs/baseline_dam3b_outputs.json` - Generated captions\n",
    "- `{PROJECT_ROOT}/results/baseline_scores.json` - Evaluation scores\n",
    "- `{PROJECT_ROOT}/results/baseline_eval_log.txt` - Full evaluation log\n",
    "\n",
    "### Next Steps:\n",
    "1. Explore attention extraction for LocalityGuard\n",
    "2. Implement multi-scale crops for Multi-Scale Attention\n",
    "3. Setup LoRA training for Region-Aware Rephraser\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Exploration (Optional)\n",
    "\n",
    "Run the cells below to understand how DAM works internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore model architecture\n",
    "print(\"DAM-3B Architecture:\\n\")\n",
    "print(f\"Vision Tower: {model.model.vision_tower.__class__.__name__}\")\n",
    "print(f\"LLM: {model.model.llm.__class__.__name__}\")\n",
    "print(f\"MM Projector: {model.model.mm_projector.__class__.__name__}\")\n",
    "\n",
    "print(\"\\nModel Components:\")\n",
    "for name, module in model.model.named_children():\n",
    "    num_params = sum(p.numel() for p in module.parameters())\n",
    "    print(f\"  {name}: {num_params / 1e9:.2f}B parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation configuration and implementation locations\n",
    "print(\"Generation Configuration:\\n\")\n",
    "print(\"Temperature: 0.2\")\n",
    "print(\"Top-p: 0.9\")\n",
    "print(\"Max tokens: 512\")\n",
    "print(\"Num beams: 1 (greedy decoding)\")\n",
    "\n",
    "print(\"\\nImplementation locations for your methods:\")\n",
    "print(\"\")\n",
    "print(\"1. LocalityGuard: Modify logits before sampling\")\n",
    "print(\"   File: describe_anything_model.py:221\")\n",
    "print(\"   Extract cross-attention from model.generate()\")\n",
    "print(\"\")\n",
    "print(\"2. Multi-Scale Attention: Extract attention from layers {3,6,9}\")\n",
    "print(\"   File: llava_arch.py (decoder forward pass)\")\n",
    "print(\"   Aggregate across focal crops at scales {1.0, 1.25, 1.5}\")\n",
    "print(\"\")\n",
    "print(\"3. Region-Aware Rephraser: LoRA on language head\")\n",
    "print(\"   Use PEFT library for LoRA\")\n",
    "print(\"   Train on small regions (area < 5%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Notebook State\n",
    "\n",
    "Save important variables before Colab disconnects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save notebook state\n",
    "state = {\n",
    "    'PROJECT_ROOT': PROJECT_ROOT,\n",
    "    'baseline_complete': True,\n",
    "    'scores': {\n",
    "        'positive': pos_score if 'pos_score' in locals() else None,\n",
    "        'negative': neg_score if 'neg_score' in locals() else None,\n",
    "        'average': avg_score if 'avg_score' in locals() else None\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f\"{PROJECT_ROOT}/notebook_state.pkl\", 'wb') as f:\n",
    "    pickle.dump(state, f)\n",
    "\n",
    "print(f\"Notebook state saved to: {PROJECT_ROOT}/notebook_state.pkl\")\n",
    "print(\"You can reload this in future sessions.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "gpuType": "A100"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
